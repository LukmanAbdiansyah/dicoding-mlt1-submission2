# -*- coding: utf-8 -*-
"""Dicoding Submission - System Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_MPz7nBw0zHZWXjIV-PzLt2oGe9htb6

**Lukman Abdiansyah**

# **Load Dataset**
"""

# install kaggle package
!pip install -q kaggle

# upload kaggle.json
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ruchi798/bookcrossing-dataset

!unzip bookcrossing-dataset.zip

import pandas as pd
summary = pd.read_csv('/content/Books Data with Category Language and Summary/Preprocessed_data.csv',index_col=[0])

summary.head()

ratings = pd.read_csv('/content/Book reviews/Book reviews/BX-Book-Ratings.csv',encoding='unicode_escape', sep=';')
ratings

users = pd.read_csv('/content/Book reviews/Book reviews/BX-Users.csv',encoding='unicode_escape', sep=';')
users

books = pd.read_csv('/content/Book reviews/Book reviews/BX_Books.csv',encoding='unicode_escape', sep=';')
books.head()

"""# **Data Understanding**"""

summary.info()

summary.isnull().sum()

summary = summary.dropna()

summary.info()

ratings.info()

ratings = ratings.drop_duplicates(['ISBN'])
ratings.head()

ratings.describe()

ratings.info()

summary = summary.drop(columns=['user_id', 'location', 'age', 'rating', 'year_of_publication', 'img_s', 'img_m', 'img_l', 'Summary','Language','city','state','country'],axis=1)
summary.head()

len(summary['Category'].unique())

summary = summary.drop_duplicates('book_title')

summary.info()

ratings.rename(columns={'ISBN': 'isbn'},inplace=True)

summary_and_rating = pd.merge(summary,ratings,on='isbn',how='left')

summary_and_rating

"""# **Data Preparation**"""

def clean_category(text):
  text = re.sub(r'[\[\]]', '', text)
  text = text.replace("'", '')
  text = text.replace('"', '')
  text = text.replace('.', '')
  return text

import re

summary_and_rating['clean_category'] = summary_and_rating['Category'].apply(clean_category)
summary_and_rating.head()

import numpy as np

category = np.sort(summary_and_rating['clean_category'].unique())
for i in category:
  print(i)

"""karena category terlalu banyak, maka untuk mempercepat komputasi dihapus kategori yang"""

dropped_category = [i for i in category if len(summary_and_rating[summary_and_rating['clean_category'] == i]) < 200 or len(summary_and_rating[summary_and_rating['clean_category'] == i]) > 800]
summary_and_rating = summary_and_rating.loc[~summary_and_rating['clean_category'].isin(dropped_category)]
summary_and_rating.head()

category = np.sort(summary_and_rating['clean_category'].unique())
for i in category:
  print(i)

summary_and_rating = summary_and_rating.drop(['Category'], axis=1)
summary_and_rating

for i in category:
  print(i, 'memiliki', len(summary_and_rating[summary_and_rating['clean_category']==i]),'data')

clean_data1 = summary_and_rating.drop(columns = ['isbn','User-ID'])
clean_data1.head()

"""# **Modeling Using Content Based Filtering**"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data kategori
tf.fit(clean_data1['clean_category'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(clean_data1['clean_category'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)

cosine_sim_df = pd.DataFrame(cosine_sim, index=clean_data1['book_title'], columns=clean_data1['book_title'])
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def book_recommendations(book_title, similarity_data=cosine_sim_df, items=clean_data1[['book_title', 'book_author', 'publisher', 'clean_category', 'Book-Rating']], k=5):
  """
  Rekomendasi Buku berdasarkan kemiripan dataframe

  Parameter:
  ---
  book_title : tipe data string (str)
              Judul Buku (index kemiripan dataframe)
  similarity_data : tipe data pd.DataFrame (object)
                    Kesamaan dataframe, simetrik, dengan book_title sebagai
                    indeks dan kolom
  items : tipe data pd.DataFrame (object)
          Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
  k : tipe data integer (int)
      Banyaknya jumlah rekomendasi yang diberikan
  ---


  Pada index ini, kita mengambil k dengan nilai similarity terbesar
  pada index matrix yang diberikan (i).
  """

  index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))
  closest = similarity_data.columns[index[-1:-(k+2):-1]]
  closest = closest.drop(book_title, errors='ignore')
  return pd.DataFrame(closest).merge(items).head(k)

clean_data1[clean_data1['book_title'].eq('From Conception to Birth: A Life Unfolds')]

recommendation = book_recommendations('From Conception to Birth: A Life Unfolds')
recommendation.sort_values(['Book-Rating'], ascending=False)

"""# **Modeling Using Collaborative Filtering**"""

# Import library
import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

tf.device('/GPU:0')

df=ratings

df.info()

# Mengubah userID menjadi list tanpa nilai yang sama
user_ids = df['User-ID'].unique().tolist()
print('list userID: ', user_ids)

# Melakukan encoding userID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID : ', user_to_user_encoded)

# Melakukan proses encoding angka ke ke userID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# Mengubah isbn menjadi list tanpa nilai yang sama
isbn_ids = df['isbn'].unique().tolist()

# Melakukan proses encoding isbn
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}

# Melakukan proses encoding angka ke isbn
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_ids)}

# Mapping userID ke dataframe user
df['user'] = df['User-ID'].map(user_to_user_encoded)

# Mapping isbn ke dataframe isbn
df['ISBN'] = df['isbn'].map(isbn_to_isbn_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# Mendapatkan jumlah buku
num_book = len(isbn_to_isbn_encoded)
print(num_book)

# Mengubah rating menjadi nilai float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['Book-Rating'])

# Nilai maksimal rating
max_rating = max(df['Book-Rating'])

print('Number of User: {}, Number of book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book, min_rating, max_rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data user dan resto menjadi satu value
x = df[['user', 'ISBN']].values

# Membuat variabel y untuk membuat rating dari hasil
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_book, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_book = num_book
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.book_embedding = layers.Embedding( # layer embeddings book
        num_book,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.book_bias = layers.Embedding(num_book, 1) # layer embedding book bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    book_vector = self.book_embedding(inputs[:, 1]) # memanggil layer embedding 3
    book_bias = self.book_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_book = tf.tensordot(user_vector, book_vector, 2)

    x = dot_user_book + user_bias + book_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size =160 ,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""# **Evaluation**"""

k = 5
threshold = 5
book_ratings = recommendation['Book-Rating'].values
book_relevances = book_ratings > threshold
precision = len(book_ratings[book_relevances]) / k
print(f'Presisi dari content based filtering sebesar {precision:.1%}')

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Evaluate the best model
loss, rmse = model.evaluate(x_val, y_val)
print('Loss:', loss)
print('RMSE:', rmse)

book_df = summary_and_rating[['isbn'	,'book_title'	,'book_author']]
df2 = ratings.sample(n=300000, random_state=42)

# Mengambil sample user
user_id = df2['User-ID'].sample(1).iloc[0]
book_read_by_user = df2[df2['User-ID'] == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
book_not_read = book_df[~book_df['isbn'].isin(book_read_by_user.ISBN.values)]['isbn']
book_not_read = list(
    set(book_not_read)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_read = [[isbn_to_isbn_encoded.get(x)] for x in book_not_read]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_read), book_not_read)
)

ratings1 = model.predict(user_book_array).flatten()

top_ratings_indices = ratings1.argsort()[-10:][::-1]
recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_read[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('Book with high ratings from user')
print('----' * 8)

top_book_user = (
    book_read_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)
    .ISBN.values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]
for row in book_df_rows.itertuples():
    print(row.book_title, 'By', row.book_author)

print('----' * 8)
print('Top 10 book recommendation')
print('----' * 8)

reccomend_book = book_df[book_df['isbn'].isin(recommended_book_ids)]
for row in reccomend_book.itertuples():
    print(row.book_title, 'By', row.book_author)